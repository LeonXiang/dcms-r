{
  "hash": "0e401d4535f8cd8a9345fe1f98ae1c37",
  "result": {
    "engine": "knitr",
    "markdown": "::: {.content-hidden}\n\\usepackage{amssymb, amsmath, amsthm, amsfonts, bm, bbm, mathrsfs} %math\n\\usepackage[italic,noendash]{mathastext} % required because of bug with mathastext vs amsmath for bold chars\n\\usepackage{mathtools}\n\n%shortcut commands for bold math\n\\newcommand{\\var}{{\\rm Var}}\n\\newcommand{\\cov}{{\\rm Cov}}\n\\newcommand{\\corr}{{\\rm Corr}}\n\n\\newcommand{\\iid}{\\stackrel{iid}{\\sim}}\n\n\\newcommand{\\bfbeta}{\\boldsymbol{\\beta}}\n\\newcommand{\\bftheta}{\\boldsymbol{\\theta}}\n\\newcommand{\\bLambda}{\\boldsymbol{\\Lambda}}\n\\newcommand{\\beps}{\\boldsymbol{\\varepsilon}}\n\n\\newcommand{\\x}{\\mathbf{x}}\n\\newcommand{\\y}{\\mathbf{y}}\n\n\n:::\n\n# Introduction {#sec-introduction}\n\nIt is instructive to start with the first sentence of @train2009 Section 1.3, \"Discrete choice analysis consists of two interrelated tasks: **specification** of the behavioral model and **estimation** of the parameters of that model\" (emphasis added). In particular, chapters 1 and 2 of @train2009 only specify models; there is not even a hint of model estimation. And since I follow @train2009, we too begin with a focus on model specification.\n\nLet me be clear: many students will see $y$ and $x$ later in this section and immediate think about \"fitting\" a regression model; that is, they assume they have data that they will put into an estimation routine to find estimates of the parameters of the model. We are not there yet! At this early stage in the book, we are only specifying models; that is, listing sets of assumptions about data generating processes and exploring the implications of those assumptions. There is no data yet. There might be parameters introduced in our choice of model, but even if so, we are not estimating those parameters yet. Have patience, we will eventually do these things.\n\n## Recap of Train Ch. 1\n\nTrain denotes the outcome in any given situation as $y$, determined by some observable factors collected in the vector $\\x$ and some unobservable factors collected in the vector $\\beps$. The factors ($\\x$ and $\\beps$) relate to the agent's choice ($y$) through a function $y = h(\\x, \\beps)$. We assume for the moment that we know $h(\\cdot)$ and that $\\x$ and $\\beps$ are length-one vectors (i.e., scalars) denoted $x$ and $\\varepsilon$.\n\nSince we do not observe $\\varepsilon$, we can't predit $y$ exactly. Instead, we focus on the probability of $y$, that is:\n\n$$\n\\begin{align}\np(y|x) \n&= \\Pr \\left( \\varepsilon \\textrm{ such that } h(x,\\varepsilon)=y \\right) \\\\\n&= \\Pr \\left( I \\left[ h(x,\\varepsilon)=y \\right] = 1 \\right) \\\\\n&= \\int I \\left[ h(x,\\varepsilon)=y \\right] f(\\varepsilon) \\, d\\varepsilon\n\\end{align}\n$$ {#eq-int_indicator}\n\nFor certain special choices of $h$ and $f$, a closed-form expression for the integral is available.[^1] But more generally, for almost any choice of $h$ and $f$, we can approximate the integral through simulation. Train provides psuedo code on how to do so:\n\n1. Repeat the following two steps many ($r=1, \\ldots, R$) times:\n    - Draw $\\varepsilon^r$ from $f(\\varepsilon)$. \n    - Determine whether $h(x,\\varepsilon^r) = y$. If so, set $I^r=1$; else set $I^r=0$. \n2. Average the $R$ values of $I^r$\n\n[^1]: In this context, a _closed-form expression_ means a way of writing the integral so that the anti-derivative sign is not part of solution. For example, the integral $\\int x \\, dx$ has the closed for expression $x^2/2$ plus some constant. We will see later that the Extreme Value distribution is often chosen for $f$ predominently because it leads to a closed for expression for the choice probability $p(y|x)$.\n\nNext we look at two examples where we use this procedure to approximate the $p(y|x)$ integral. \n\n\n## A Simple Example {#sec-simple_example}\n\nLet's first set up a toy example to demonstrate how simulation can approximate the $p(y|x)$ integral. Suppose $x=0.5$ and $\\varepsilon$ is uniformly distributed between $-1$ and $1$. Define $h(x, \\varepsilon)$ to be:\n\n$$\nh(x, \\varepsilon) = \n    \\begin{cases}\n        0  & \\text{if } x + \\varepsilon < 0 \\\\\n        1  & \\text{if } x + \\varepsilon \\in [0,1] \\\\\n        2  & \\text{if } x + \\varepsilon > 1\n    \\end{cases}\n$$ {#eq-h_uniform}\n\nWe'll focus on the outcome $y=2$. You can probably intuit that the $p(y=2 | x) = 0.25$ since only one quarter of the time will $\\varepsilon$ be sufficiently positive to make $x + \\varepsilon > 1$.[^2] Nevertheless, let's approximate the integral representation of $p(y=2|x)$ through simulation to ensure we understand the process.\n\n[^2]: More precisely, $p(y=2|x) = \\Pr(\\varepsilon > 0.5) = \\int_{0.5}^1 f(\\varepsilon) d\\varepsilon = (0.5)\\vert_{0.5}^1 = 0.25$.\n\nTo walk you through the code, we first set a seed so that the pseudo-random numbers generated by `runif()` can be replicated exactly each time the code is run (even on different computers). We then specify that we will use 1,000 draws in the simulation and we create an vector `I` to hold our results. The simulation occurs via a `for()` loop where each time through the loop we take a draw of $\\varepsilon$, calculate $0.5 + \\varepsilon$ and check whether that sum is greater than one. If so, then $h(x,\\varepsilon)=2$ matching the value of $y$ for the choice probability we want to assess --- i.e., $p(y=2|x)$ --- and thus we store a $1$ in the $r^\\textrm{th}$ position of `I`; otherwise we store a 0. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\n\nR <- 1000\nI <- vector(length=R)\n\nfor(r in 1:R) {\n    eps  <- runif(1, min=-1, max=1)\n    h    <- 0.5 + eps \n    I[r] <- as.integer(h > 1)\n}\nmean(I)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.258\n```\n\n\n:::\n:::\n\n\n\nThe simulated value 0.258 is close to exact value 0.25 and can be made closer by increasing the number of draws used in the simulation.\n\nR users will recognize that we can simplify the code by taking advantage of R's vectorized functions and its conversion of boolean values to 0/1 when used in mathematical operations. Here is a shorter implementation of the simulation; whether it's \"better\" code is a matter of preference.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(1234)\nR <- 1000\nmean( runif(R, min=-1, max=1) + 0.5 > 1 )\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.258\n```\n\n\n:::\n:::\n\n\n\nThat's it. If you can generate pseudo-random draws from the density $f$ and you know $h$, approximating a choice probability by simulation requires only a handful of lines of code.\n\n\n## A Binary Logit Model Example {#sec-binary_logit}\n\nAs an example of a model with a complete closed-form solution, Train provides the binary logit model. \n\nThe \"binary\" part refers to the aspect of the model whereby the decision maker does one of two things; they either take an action $y=1$ or not $y=0$. To tie this model into a framework of behavior, we start with a utility function $U$. In Train's specific example, utility is specified as \n\n$$\nU(\\x, \\bfbeta, \\varepsilon) = \\x'\\bfbeta + \\varepsilon\n$$ {#eq-u_binlogit}\n\nwhere $\\x$ is a vector of observable information, $\\bfbeta$ is a vector of parameters that through the functional form $\\x'\\bfbeta$ effectively serve as weights on the $\\x$ variables, and $\\varepsilon$ is a scalar index collecting the value of information used by the decision maker but unobserved to the researcher. \n\nIn this model, the threshold for action is 0. That is, we can specify $h$ as:\n\n$$\nh(\\x, \\bfbeta, \\varepsilon) = \n    \\begin{cases}\n        0  & \\text{if } U \\le 0 \\\\\n        1  & \\text{if } U > 0 \n    \\end{cases}\n$$ {#eq-h_binlogit}\n\nThe \"logit\" part of the model's name refers to the choice of $f$. We have assumed $f$ is the logistic distribution: \n\n$$\nf(\\varepsilon) = \\frac{e^{-\\varepsilon}}{(1+e^{-\\varepsilon})^2}\n$$ {#eq-bin_logistic_dist}\n\nHaving specified $h$ and $f$, let's choose some values for $\\x$ and $\\bfbeta$ and use simulation to approximate the integral for $p(y|\\x,\\bfbbeta)$. Let's pick $\\x=(0.5, 2)$ and $\\bfbeta=(3,-1)$. We know from the closed-form solution to this model that, with these values of $\\x$ and $\\bfbeta$, the probability the decision maker takes action is:\n\n$$\np(y=1 | \\x, \\bfbeta) = \\frac{e^{\\bfbeta'\\x}}{1 + e^{\\bfbeta'\\x}} = \\frac{e^{-0.5}}{1+e^{-0.5}} = 0.3775407\n$$ {#eq-binlogit_example}\n\nWe can approximate this integral as before. Below I use `rlogis()` to take `R=1,000` draws from the binary logistic distribution, and we approximate the integral with the proportion of times $\\x'\\bfbeta + \\varepsilon > 0$:\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(2345)\nR <- 1000\n\nx    <- c(0.5, 2)\nbeta <- c(3, -1)\n\nU <- as.vector(x %*% beta) + rlogis(R)\nmean(U > 0)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] 0.364\n```\n\n\n:::\n:::\n\n\n\nOur simulated value 0.364 approximates the exact value of the integral 0.378.\n\n## Key Learnings\n\nThe core learning from this chapter is that with discrete choice models our focus is on the _probability_ of outcome $y$. That outcome result from the distribution of unobserved factors $\\beps$ and the behavior model $h$ that relates $y$ to $(\\x, \\beps)$. The probability $p(y|x)$ can occasionally and for only very special choices of $f$ and $h$ be written in closed form, but for almost any choice of $f$ and $h$ we can simulate $p(y|x)$.",
    "supporting": [
      "01_intro_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {
      "knitr": [
        "{\"type\":\"list\",\"attributes\":{},\"value\":[]}"
      ]
    },
    "preserve": null,
    "postProcess": false
  }
}
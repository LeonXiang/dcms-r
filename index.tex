% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \ifXeTeX
    \usepackage{mathspec} % this also loads fontspec
  \else
    \usepackage{unicode-math} % this also loads fontspec
  \fi
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\usepackage{amssymb, amsmath, amsthm, amsfonts, bm, bbm, mathrsfs} %math
\usepackage[italic,noendash]{mathastext} % required because of bug with mathastext vs amsmath for bold chars
\usepackage{mathtools}

%shortcut commands for bold math
\newcommand{\var}{{\rm Var}}
\newcommand{\cov}{{\rm Cov}}
\newcommand{\corr}{{\rm Corr}}

\newcommand{\iid}{\stackrel{iid}{\sim}}

\newcommand{\bfbeta}{\boldsymbol{\beta}}
\newcommand{\bftheta}{\boldsymbol{\theta}}
\newcommand{\bLambda}{\boldsymbol{\Lambda}}
\newcommand{\beps}{\boldsymbol{\varepsilon}}

\newcommand{\x}{\mathbf{x}}
\newcommand{\y}{\mathbf{y}}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={R Companion to Discrete Choice Methods with Simulation},
  pdfauthor={Dan Yavorsky},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}

\title{R Companion to Discrete Choice Methods with Simulation}
\author{Dan Yavorsky}
\date{}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}
\bookmarksetup{startatroot}

\chapter*{Welcome}\label{welcome}
\addcontentsline{toc}{chapter}{Welcome}

\markboth{Welcome}{Welcome}

\begin{tcolorbox}[enhanced jigsaw, opacityback=0, rightrule=.15mm, colframe=quarto-callout-warning-color-frame, bottomrule=.15mm, colback=white, breakable, colbacktitle=quarto-callout-warning-color!10!white, left=2mm, leftrule=.75mm, coltitle=black, bottomtitle=1mm, toptitle=1mm, titlerule=0mm, toprule=.15mm, arc=.35mm, opacitybacktitle=0.6, title=\textcolor{quarto-callout-warning-color}{\faExclamationTriangle}\hspace{0.5em}{Warning}]

This is an early draft. The book is incomplete. Every aspect of it is
subject to change. Many things might be incorrect. Are you sure you want
to read this yet?

\end{tcolorbox}

\begin{quote}
You don't understand it, until you have coded it.
\end{quote}

It's a mantra that my mentors instilled in me in graduate school, and
one that I propogate on my students today. I believe it in deeply. If
you want to understand a statistical model, it is insufficient to
interact with the model only through pen and paper, no matter how well
you organize the subscripts or how masterfully you interweave elements
of the Greek and Roman alphabets. To really understand a statistical
model, you need to be able to simulate data from the model, and then
take that simulated data to an estimation routine that enables you to
recover the parameters of interest.

This is something you learn to do in graduate school. But, unless you
have one of the most caring and pedagogical advisors, no one teaches it
to you! Instead, it's a skill that students develop independently and
inefficiently between classes and assignment due dates or, in my case,
after I was knee-deep in my dissertation research. There is a tremendous
imbalance between how immensely important this skill is and the lack of
time and instruction dedicated to it.\footnote{In this context, a
  \emph{closed-form expression} means a way of writing the integral so
  that the anti-derivative sign is not part of solution. For example,
  the integral \(\int x \, dx\) has the closed for expression \(x^2/2\)
  plus some constant. We will see later that the Extreme Value
  distribution is often chosen for \(f\) predominently because it leads
  to a closed for expression for the choice probability \(p(y|x)\).}
That imbalance is the motivation for this book.

Indeed, Train and I align in our thinking: ``\ldots the true value of
the new approach to choice modeling is the ability to create tailor-made
models. The computation and programming steps that are needed to
implement a new model are usually not difficult. An important goal of
the book is to teach these skills as an integral part of the exposition
of the models themselves.''\footnote{More precisely,
  \(p(y=2|x) = \Pr(\varepsilon > 0.5) = \int_{0.5}^1 f(\varepsilon) d\varepsilon = (0.5)\vert_{0.5}^1 = 0.25\).}
To that end, Train provides pseudo-code throughout his book and
generously makes Matlab code available on his website.

This text takes it a step further. What is ``not that difficult'' for an
experienced researcher can be immensely difficult for a typical graduate
student. The aim of this book is to ease that difficulty. I demonstrate
R code alongside the math. You will see Train's pseudo-code transformed
into working R code. No mysteries will remain. To quote Quintilian,
Cobbett, Cooke, and many others: my goal is to take you through the
process of programming the simulation and estimation of discrete choice
models not so that you can understand, but so that you cannot possibly
misunderstand.

\section*{How to read this book}\label{how-to-read-this-book}
\addcontentsline{toc}{section}{How to read this book}

\markright{How to read this book}

As the title suggests, the structure, topics, and organization of this
book parallel Kenneth Train's masterful text \emph{Discrete Choice
Methods with Simulation} (Second Edition) freely available online at
\url{https://eml.berkeley.edu/books/choice2.html}.

You should read --- or most likely, re-read --- one chapter from Train
(2009) and then read and work through the corresponding chapter here,
alternating between our texts. Do not simply read this book from start
to finish without frequenly returning to Train (2009). Doing so only
deprives you of the intended experience and likely dramatically reduces
the amount you will learn from the process.

In addition, I strongly encourage you write or copy the code as you work
though this book. I would mandate this if I could, but I'm not sitting
there next to you and so I must settle for simply sharing my
encouragement. Write the code. Play with the code. Break the code. Make
it your own. That behavior is where deep understanding comes from, not
from highlighting the occasional sentence or equation you find
important.

\section*{Prerequisites}\label{prerequisites}
\addcontentsline{toc}{section}{Prerequisites}

\markright{Prerequisites}

This book is intended for a narrow audience, predominently graduate
students with an interest in discrete choice modelling who will find
value from seeing and interacting with the programmatic implementation
of the multinomial logit and its extended family of related models. In
other words, someone who read Train (2009) and thought \emph{how would I
code that?}

We will simulate data from the statistical models and then estimate the
parameters of those models from the simulated data. We will do all of
this in R, a freely available software environment for statistical
computing and graphics. As a result, I assume you are reasonably
familiar with R. If not, there is a tremendous set of free online R
resources collected and organized at \url{https://www.bigbookofr.com/}.
Popular books include \href{https://r4ds.hadley.nz/}{R for Data Science}
and \href{https://adv-r.hadley.nz/}{Advanced R}.

I also assume you have taken introductory statistics or econometrics
courses, as the concepts and techniques taught there are foundational
for understanding and estimating the discrete choice models covered by
Train (2009). In particular, you should have no uncertainty about the
difference between a model, the estimator, and the estimate. To briefly
review:

\begin{itemize}
\tightlist
\item
  a \emph{model} is the set of mathematical assumptions about how data
  are generated,
\item
  the \emph{estimator} (or equivalently, the estimation routine) is an
  algorithm or function of the data, and
\item
  the \emph{estimate} is the result of applying the estimator to a
  particular dataset.
\end{itemize}

In my experience, students are often given a model and estimator, after
which much time in the classroom is spent deriving properties of the
estimator for that particular model, and then a homework assignment asks
students to implement the estimator on a dataset to find an estimate.
This approach puts almost no emphasis on the specification of the model
or the choice of estimator. For example, should part of the model be
specified as \(\beta_0 + \beta_1x_1\) or should it be
\(\beta_0 + \beta_1x_1 + \beta_2x_2\)? And once we have specified a
model, should we estimate it via least squares, the method of moments,
maximum likelihood, a Bayesian approach, or some other way?

If my description captures your experience in introductory statistics
and econometrics courses and you would like to review key ideas, I
highly recommend
\href{https://www.routledge.com/Statistical-Theory-A-Concise-Introduction/Abramovich-Ritov/p/book/9781032007458}{Abramovich
\& Ritov} on the topic of mathematical statistics,
\href{https://www.amazon.com/Course-Econometrics-Arthur-S-Goldberger/dp/0674175441}{Golderberger}
and
\href{https://www.amazon.com/Guide-Econometrics-6th-Peter-Kennedy/dp/1405182571/}{Kennedy}
for econometrics, and
\href{https://xcelab.net/rm/statistical-rethinking/}{McElreath} for
Bayesian statistics.

\section*{Acknowledgments}\label{acknowledgments}
\addcontentsline{toc}{section}{Acknowledgments}

\markright{Acknowledgments}

I am immensely greatful to the teams that work on the R-Project, those
at Posit who provide RStudio and Quarto, and the related communities of
developers, academics, and R users. The free tools they provide and the
welcoming communities they have established are both exceptional.

I also thank my academic mentors Ella Honka, Peter Rossi, Eric Bradlow,
and (although we have yet to meet) Kenneth Train as well as the folks
that tolerate me through my consulting work, including Prachi Bhalerao,
June Wu, Chris Diener, and Keith Chrzan. I have learned so much from
their guidance and often their written work.

Special thanks go to generous people who reviewed and assisted with
drafts of this book, including Darren Aeillo, Kalyan Rallabandi, and
Geoff Zheng.

Unquestionably, my deepest thanks go to Alison, Zach, and Ben for their
patience and support.

\section*{Colophon}\label{colophon}
\addcontentsline{toc}{section}{Colophon}

\markright{Colophon}

An online version of this book is available at
\url{https://dcms-r.danyavorsky.com}. The source of the book is
available at \url{https://github.com/dyavorsky/dcms-r}. The book is
authored using \href{https://quarto.org}{Quarto}, an open-source
scientific and technical publishing system that makes it easy to create
articles, presentations, websites, books, and other publications that
combine text and executible code.

\part{Introduction}

\chapter{Introduction}\label{sec-introduction}

It is instructive to start with the first sentence of Train (2009)
Section 1.3, ``Discrete choice analysis consists of two interrelated
tasks: \textbf{specification} of the behavioral model and
\textbf{estimation} of the parameters of that model'' (emphasis added).
In particular, chapters 1 and 2 of Train (2009) only specify models;
there is not even a hint of model estimation. And since I follow Train
(2009), we too begin with a focus on model specification.

Let me be clear: many students will see \(y\) and \(x\) later in this
section and immediate think about ``fitting'' a regression model; that
is, they assume they have data that they will put into an estimation
routine to find estimates of the parameters of the model. We are not
there yet! At this early stage in the book, we are only specifying
models; that is, listing sets of assumptions about data generating
processes and exploring the implications of those assumptions. There is
no data yet. There might be parameters introduced in our choice of
model, but even if so, we are not estimating those parameters yet. Have
patience, we will eventually do these things.

\section{Recap of Train Ch. 1}\label{recap-of-train-ch.-1}

Train denotes the outcome in any given situation as \(y\), determined by
some observable factors collected in the vector \(\mathbf{x}\) and some
unobservable factors collected in the vector
\(\boldsymbol{\varepsilon}\). The factors (\(\mathbf{x}\) and
\(\boldsymbol{\varepsilon}\)) relate to the agent's choice (\(y\))
through a function \(y = h(\mathbf{x}, \boldsymbol{\varepsilon})\). We
assume for the moment that we know \(h(\cdot)\) and that \(\mathbf{x}\)
and \(\boldsymbol{\varepsilon}\) are length-one vectors (i.e., scalars)
denoted \(x\) and \(\varepsilon\).

Since we do not observe \(\varepsilon\), we can't predit \(y\) exactly.
Instead, we focus on the probability of \(y\), that is:

\begin{equation}\phantomsection\label{eq-int_indicator}{
\begin{align}
p(y|x) 
&= \Pr \left( \varepsilon \textrm{ such that } h(x,\varepsilon)=y \right) \\
&= \Pr \left( I \left[ h(x,\varepsilon)=y \right] = 1 \right) \\
&= \int I \left[ h(x,\varepsilon)=y \right] f(\varepsilon) \, d\varepsilon
\end{align}
}\end{equation}

For certain special choices of \(h\) and \(f\), a closed-form expression
for the integral is available.\footnote{In this context, a
  \emph{closed-form expression} means a way of writing the integral so
  that the anti-derivative sign is not part of solution. For example,
  the integral \(\int x \, dx\) has the closed for expression \(x^2/2\)
  plus some constant. We will see later that the Extreme Value
  distribution is often chosen for \(f\) predominently because it leads
  to a closed for expression for the choice probability \(p(y|x)\).} But
more generally, for almost any choice of \(h\) and \(f\), we can
approximate the integral through simulation. Train provides psuedo code
on how to do so:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Repeat the following two steps many (\(r=1, \ldots, R\)) times:

  \begin{itemize}
  \tightlist
  \item
    Draw \(\varepsilon^r\) from \(f(\varepsilon)\).
  \item
    Determine whether \(h(x,\varepsilon^r) = y\). If so, set \(I^r=1\);
    else set \(I^r=0\).
  \end{itemize}
\item
  Average the \(R\) values of \(I^r\)
\end{enumerate}

Next we look at two examples where we use this procedure to approximate
the \(p(y|x)\) integral.

\section{A Simple Example}\label{sec-simple_example}

Let's first set up a toy example to demonstrate how simulation can
approximate the \(p(y|x)\) integral. Suppose \(x=0.5\) and
\(\varepsilon\) is uniformly distributed between \(-1\) and \(1\).
Define \(h(x, \varepsilon)\) to be:

\begin{equation}\phantomsection\label{eq-h_uniform}{
h(x, \varepsilon) = 
    \begin{cases}
        0  & \text{if } x + \varepsilon < 0 \\
        1  & \text{if } x + \varepsilon \in [0,1] \\
        2  & \text{if } x + \varepsilon > 1
    \end{cases}
}\end{equation}

We'll focus on the outcome \(y=2\). You can probably intuit that the
\(p(y=2 | x) = 0.25\) since only one quarter of the time will
\(\varepsilon\) be sufficiently positive to make
\(x + \varepsilon > 1\).\footnote{More precisely,
  \(p(y=2|x) = \Pr(\varepsilon > 0.5) = \int_{0.5}^1 f(\varepsilon) d\varepsilon = (0.5)\vert_{0.5}^1 = 0.25\).}
Nevertheless, let's approximate the integral representation of
\(p(y=2|x)\) through simulation to ensure we understand the process.

To walk you through the code, we first set a seed so that the
pseudo-random numbers generated by \texttt{runif()} can be replicated
exactly each time the code is run (even on different computers). We then
specify that we will use 1,000 draws in the simulation and we create an
vector \texttt{I} to hold our results. The simulation occurs via a
\texttt{for()} loop where each time through the loop we take a draw of
\(\varepsilon\), calculate \(0.5 + \varepsilon\) and check whether that
sum is greater than one. If so, then \(h(x,\varepsilon)=2\) matching the
value of \(y\) for the choice probability we want to assess --- i.e.,
\(p(y=2|x)\) --- and thus we store a \(1\) in the \(r^\textrm{th}\)
position of \texttt{I}; otherwise we store a 0.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}

\NormalTok{R }\OtherTok{\textless{}{-}} \DecValTok{1000}
\NormalTok{I }\OtherTok{\textless{}{-}} \FunctionTok{vector}\NormalTok{(}\AttributeTok{length=}\NormalTok{R)}

\ControlFlowTok{for}\NormalTok{(r }\ControlFlowTok{in} \DecValTok{1}\SpecialCharTok{:}\NormalTok{R) \{}
\NormalTok{    eps  }\OtherTok{\textless{}{-}} \FunctionTok{runif}\NormalTok{(}\DecValTok{1}\NormalTok{, }\AttributeTok{min=}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{max=}\DecValTok{1}\NormalTok{)}
\NormalTok{    h    }\OtherTok{\textless{}{-}} \FloatTok{0.5} \SpecialCharTok{+}\NormalTok{ eps }
\NormalTok{    I[r] }\OtherTok{\textless{}{-}} \FunctionTok{as.integer}\NormalTok{(h }\SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{)}
\NormalTok{\}}
\FunctionTok{mean}\NormalTok{(I)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.258
\end{verbatim}

The simulated value 0.258 is close to exact value 0.25 and can be made
closer by increasing the number of draws used in the simulation.

R users will recognize that we can simplify the code by taking advantage
of R's vectorized functions and its conversion of boolean values to 0/1
when used in mathematical operations. Here is a shorter implementation
of the simulation; whether it's ``better'' code is a matter of
preference.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{1234}\NormalTok{)}
\NormalTok{R }\OtherTok{\textless{}{-}} \DecValTok{1000}
\FunctionTok{mean}\NormalTok{( }\FunctionTok{runif}\NormalTok{(R, }\AttributeTok{min=}\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{, }\AttributeTok{max=}\DecValTok{1}\NormalTok{) }\SpecialCharTok{+} \FloatTok{0.5} \SpecialCharTok{\textgreater{}} \DecValTok{1}\NormalTok{ )}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.258
\end{verbatim}

That's it. If you can generate pseudo-random draws from the density
\(f\) and you know \(h\), approximating a choice probability by
simulation requires only a handful of lines of code.

\section{A Binary Logit Model Example}\label{sec-binary_logit}

As an example of a model with a complete closed-form solution, Train
provides the binary logit model.

The ``binary'' part refers to the aspect of the model whereby the
decision maker does one of two things; they either take an action
\(y=1\) or not \(y=0\). To tie this model into a framework of behavior,
we start with a utility function \(U\). In Train's specific example,
utility is specified as

\begin{equation}\phantomsection\label{eq-u_binlogit}{
U(\mathbf{x}, \boldsymbol{\beta}, \varepsilon) = \mathbf{x}'\boldsymbol{\beta}+ \varepsilon
}\end{equation}

where \(\mathbf{x}\) is a vector of observable information,
\(\boldsymbol{\beta}\) is a vector of parameters that through the
functional form \(\mathbf{x}'\boldsymbol{\beta}\) effectively serve as
weights on the \(\mathbf{x}\) variables, and \(\varepsilon\) is a scalar
index collecting the value of information used by the decision maker but
unobserved to the researcher.

In this model, the threshold for action is 0. That is, we can specify
\(h\) as:

\begin{equation}\phantomsection\label{eq-h_binlogit}{
h(\mathbf{x}, \boldsymbol{\beta}, \varepsilon) = 
    \begin{cases}
        0  & \text{if } U \le 0 \\
        1  & \text{if } U > 0 
    \end{cases}
}\end{equation}

The ``logit'' part of the model's name refers to the choice of \(f\). We
have assumed \(f\) is the logistic distribution:

\begin{equation}\phantomsection\label{eq-bin_logistic_dist}{
f(\varepsilon) = \frac{e^{-\varepsilon}}{(1+e^{-\varepsilon})^2}
}\end{equation}

Having specified \(h\) and \(f\), let's choose some values for
\(\mathbf{x}\) and \(\boldsymbol{\beta}\) and use simulation to
approximate the integral for \(p(y|\mathbf{x},\bfbbeta)\). Let's pick
\(\mathbf{x}=(0.5, 2)\) and \(\boldsymbol{\beta}=(3,-1)\). We know from
the closed-form solution to this model that, with these values of
\(\mathbf{x}\) and \(\boldsymbol{\beta}\), the probability the decision
maker takes action is:

\begin{equation}\phantomsection\label{eq-binlogit_example}{
p(y=1 | \mathbf{x}, \boldsymbol{\beta}) = \frac{e^{\boldsymbol{\beta}'\mathbf{x}}}{1 + e^{\boldsymbol{\beta}'\mathbf{x}}} = \frac{e^{-0.5}}{1+e^{-0.5}} = 0.3775407
}\end{equation}

We can approximate this integral as before. Below I use
\texttt{rlogis()} to take \texttt{R=1,000} draws from the binary
logistic distribution, and we approximate the integral with the
proportion of times \(\mathbf{x}'\boldsymbol{\beta}+ \varepsilon > 0\):

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{2345}\NormalTok{)}
\NormalTok{R }\OtherTok{\textless{}{-}} \DecValTok{1000}

\NormalTok{x    }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\DecValTok{2}\NormalTok{)}
\NormalTok{beta }\OtherTok{\textless{}{-}} \FunctionTok{c}\NormalTok{(}\DecValTok{3}\NormalTok{, }\SpecialCharTok{{-}}\DecValTok{1}\NormalTok{)}

\NormalTok{U }\OtherTok{\textless{}{-}} \FunctionTok{as.vector}\NormalTok{(x }\SpecialCharTok{\%*\%}\NormalTok{ beta) }\SpecialCharTok{+} \FunctionTok{rlogis}\NormalTok{(R)}
\FunctionTok{mean}\NormalTok{(U }\SpecialCharTok{\textgreater{}} \DecValTok{0}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.364
\end{verbatim}

Our simulated value 0.364 approximates the exact value of the integral
0.378.

\section{Key Learnings}\label{key-learnings}

The core learning from this chapter is that with discrete choice models
our focus is on the \emph{probability} of outcome \(y\). That outcome
result from the distribution of unobserved factors
\(\boldsymbol{\varepsilon}\) and the behavior model \(h\) that relates
\(y\) to \((\mathbf{x}, \boldsymbol{\varepsilon})\). The probability
\(p(y|x)\) can occasionally and for only very special choices of \(f\)
and \(h\) be written in closed form, but for almost any choice of \(f\)
and \(h\) we can simulate \(p(y|x)\).

\chapter{Properties of Discrete Choice Models}\label{sec-properties}

add

\part{Behavioral Models}

\chapter{Logit}\label{sec-logit}

add

\chapter{GEV}\label{sec-gev}

add

\chapter{Probit}\label{sec-probit}

add

\chapter{Mixed Logit}\label{sec-mixedlogit}

add

\begin{verbatim}


`<!-- quarto-file-metadata: eyJyZXNvdXJjZURpciI6ImNoYXB0ZXJzIn0= -->`{=html}

```{=html}
<!-- quarto-file-metadata: eyJyZXNvdXJjZURpciI6ImNoYXB0ZXJzIiwiYm9va0l0ZW1UeXBlIjoiY2hhcHRlciIsImJvb2tJdGVtTnVtYmVyIjo3LCJib29rSXRlbUZpbGUiOiJjaGFwdGVycy8wN192YXJpYXRpb25zLnFtZCIsImJvb2tJdGVtRGVwdGgiOjF9 -->
\end{verbatim}

\chapter{Variation on a Theme}\label{sec-variations}

add

\chapter{Numerical Maximization}\label{sec-mazimization}

add

\part{Estimation}

\chapter{Drawing from Densities}\label{sec-densities}

add

\chapter{Simulation-Assisted Estimation}\label{sec-sim_estimation}

add

\chapter{Individual-Level Parameters}\label{sec-indiv_parameters}

add

\chapter{Bayesian Procedures}\label{sec-bayesian}

add

\chapter{Endogeneity}\label{sec-endogeneity}

add

\chapter{EM Algorithms}\label{sec-em_algo}

add

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-train2009}
Train, Kenneth E. 2009. \emph{Discrete Choice Methods with Simulation}.
Cambridge University Press.
\url{https://eml.berkeley.edu/books/choice2.html}.

\end{CSLReferences}

\cleardoublepage
\phantomsection
\addcontentsline{toc}{part}{Appendices}
\appendix

\chapter{Appendix}\label{sec-appendix}

add



\end{document}
